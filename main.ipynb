{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a40ca44c",
   "metadata": {},
   "source": [
    "The Rice Dataset (Raw Images) has 38 classes and each class have 500 images. Which are loaded here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf673ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/kaggle/input/riceds-original/Original'\n",
    "subdirs = sorted(os.listdir(base_dir))\n",
    "print(f'Found {len(subdirs)} class-folders, e.g.:', subdirs[:5])\n",
    "\n",
    "# Count files in each\n",
    "for d in subdirs[:5]:\n",
    "    n = len(os.listdir(os.path.join(base_dir, d)))\n",
    "    print(f'  {d}: {n} files')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa764fef",
   "metadata": {},
   "source": [
    "Here we are checking whether the images are loaded correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "base_dir = '/kaggle/input/riceds-original/Original'\n",
    "\n",
    "# Pick 3 class‚Äêfolders √ó 3 images each = 9 samples\n",
    "classes = sorted(os.listdir(base_dir))[:3]\n",
    "images, labels = [], []\n",
    "for cls in classes:\n",
    "    cls_path = os.path.join(base_dir, cls)\n",
    "    for fn in sorted(os.listdir(cls_path))[:3]:\n",
    "        img = Image.open(os.path.join(cls_path, fn))\n",
    "        images.append(img)\n",
    "        labels.append(cls)\n",
    "\n",
    "# Plot 3√ó3 grid\n",
    "fig, axes = plt.subplots(3, 3, figsize=(6, 6))\n",
    "for ax, img, lbl in zip(axes.flatten(), images, labels):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(lbl)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef753b",
   "metadata": {},
   "source": [
    "A peak toward the left means most pixels are dark‚Äîyour images may be under-exposed or generally low-contrast.\n",
    "\n",
    "A peak toward the right suggests brighter images‚Äîpossibly over-exposed or with lots of light backgrounds.\n",
    "\n",
    "A wide spread (values across the whole 0‚Äì255 range) indicates good contrast.\n",
    "\n",
    "A narrow, tall spike (e.g. around 120-140) shows most of your pixels cluster around a medium gray‚Äîyour images may look ‚Äúflat.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sample a subset of images\n",
    "pix_vals = []\n",
    "for cls in classes[:5]:          # for speed, just first 5 classes\n",
    "    fns = os.listdir(os.path.join(base_dir, cls))[:20]\n",
    "    for fn in fns:\n",
    "        arr = np.array(Image.open(os.path.join(base_dir, cls, fn)).convert('L'))\n",
    "        pix_vals.extend(arr.flatten())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(pix_vals, bins=50)\n",
    "plt.title('Grayscale Pixel Distribution (sampled)')\n",
    "plt.xlabel('Pixel value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd766da",
   "metadata": {},
   "source": [
    "full PyTorch example showing how to fine-tune a pretrained ResNet50 on your 38-class rice dataset, with simple augmentations and a two-stage training (head first, then full-fine-tune) to help boost accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ae16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Paths & hyper-parameters\n",
    "base_dir    = '/kaggle/input/riceds-original/Original'\n",
    "batch_size  = 32\n",
    "num_epochs  = 20\n",
    "num_classes = 38\n",
    "device      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2. Data transforms\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# 3. Dataset & split\n",
    "full_dataset = datasets.ImageFolder(base_dir, transform=train_tf)\n",
    "train_size   = int(0.8 * len(full_dataset))\n",
    "val_size     = len(full_dataset) - train_size\n",
    "train_ds, val_ds = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "val_ds.dataset.transform = val_tf\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# 4. Model setup\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt_head = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "opt_full = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-4)\n",
    "sched    = optim.lr_scheduler.StepLR(opt_full, step_size=7, gamma=0.1)\n",
    "\n",
    "# 5. Training & validation loops\n",
    "def run_epoch(loader, model, criterion, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    loop = tqdm(loader, desc='Train' if is_train else ' Val ')\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for x, y in loop:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(x)\n",
    "            loss   = criterion(logits, y)\n",
    "            preds  = torch.argmax(logits, dim=1)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        running_loss    += loss.item() * x.size(0)\n",
    "        running_corrects+= (preds == y).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc  = running_corrects / len(loader.dataset)\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# 6. Stage 1: Train head only\n",
    "print(\"\\n=== Stage 1: training head ===\")\n",
    "for epoch in range(3):\n",
    "    tl, ta = run_epoch(train_loader, model, criterion, opt_head)\n",
    "    vl, va = run_epoch(val_loader,   model, criterion, None)\n",
    "    print(f\"[Head] Epoch {epoch+1}/3  train_loss={tl:.3f} train_acc={ta:.3f}  val_loss={vl:.3f} val_acc={va:.3f}\")\n",
    "\n",
    "# 7. Stage 2: Fine-tune all layers\n",
    "print(\"\\n=== Stage 2: fine-tuning full model ===\")\n",
    "for epoch in range(num_epochs):\n",
    "    tl, ta = run_epoch(train_loader, model, criterion, opt_full)\n",
    "    sched.step()\n",
    "    vl, va = run_epoch(val_loader,   model, criterion, None)\n",
    "    print(f\"[Full] Epoch {epoch+1}/{num_epochs}  train_loss={tl:.3f} train_acc={ta:.3f}  val_loss={vl:.3f} val_acc={va:.3f}\")\n",
    "\n",
    "# 8. Save best model\n",
    "torch.save(model.state_dict(), 'rice_resnet50_finetuned.pt')\n",
    "\n",
    "# 9. Evaluation Metrics\n",
    "print(\"\\n=== Evaluation Metrics on Validation Set ===\")\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(val_loader, desc='Eval metrics'):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        preds  = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(y.cpu().numpy())\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "# Concatenate results\n",
    "all_preds   = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n",
    "all_probs   = np.concatenate(all_probs)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, digits=4))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# AUC Score (One-vs-Rest)\n",
    "try:\n",
    "    auc_score = roc_auc_score(all_targets, all_probs, multi_class='ovr')\n",
    "    print(f\"\\nAUC Score (macro OVR): {auc_score:.4f}\")\n",
    "except ValueError as e:\n",
    "    print(f\"\\nAUC Score could not be computed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1422286",
   "metadata": {},
   "source": [
    "BYOL implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "base_dir = '/kaggle/input/riceds-original/Original'\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "NUM_CLASSES = 38\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Augmentations for BYOL\n",
    "class CustomBYOLTransform:\n",
    "    def __init__(self, size=224):\n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                         [0.229, 0.224, 0.225])\n",
    "        self.transform1 = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9),\n",
    "            transforms.ToTensor(), normalize\n",
    "        ])\n",
    "        self.transform2 = transforms.Compose(self.transform1.transforms)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.transform1(x), self.transform2(x)\n",
    "\n",
    "# Dataset class with dual views\n",
    "class ImageFolderBYOL(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        path, _ = self.samples[index]\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        return self.transform(img)\n",
    "\n",
    "# Load BYOL training data\n",
    "full_dataset = ImageFolderBYOL(base_dir, transform=CustomBYOLTransform())\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "train_dataset_byol, _ = random_split(full_dataset, [train_size, len(full_dataset) - train_size])\n",
    "train_loader_byol = DataLoader(train_dataset_byol, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# Evaluation transform and data\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256), transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "eval_dataset = datasets.ImageFolder(base_dir, transform=eval_tf)\n",
    "train_eval_size = int(0.8 * len(eval_dataset))\n",
    "train_dataset_eval, test_dataset_eval = random_split(eval_dataset, [train_eval_size, len(eval_dataset) - train_eval_size])\n",
    "train_loader_eval = DataLoader(train_dataset_eval, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_eval = DataLoader(test_dataset_eval, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model components\n",
    "class BYOLResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = models.resnet50(pretrained=False)\n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.feature_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x).view(x.size(0), -1)\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.BatchNorm1d(hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.BatchNorm1d(hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class BYOL(nn.Module):\n",
    "    def __init__(self, encoder, proj_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.online_encoder = encoder\n",
    "        self.online_projector = ProjectionHead(encoder.feature_dim, hidden_dim, proj_dim)\n",
    "        self.online_predictor = PredictionHead(proj_dim, hidden_dim, proj_dim)\n",
    "\n",
    "        self.target_encoder = copy.deepcopy(encoder)\n",
    "        self.target_projector = copy.deepcopy(self.online_projector)\n",
    "        for p in self.target_encoder.parameters(): p.requires_grad = False\n",
    "        for p in self.target_projector.parameters(): p.requires_grad = False\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.online_projector(self.online_encoder(x1))\n",
    "        z2 = self.online_projector(self.online_encoder(x2))\n",
    "        p1 = self.online_predictor(z1)\n",
    "        p2 = self.online_predictor(z2)\n",
    "        with torch.no_grad():\n",
    "            t1 = self.target_projector(self.target_encoder(x1))\n",
    "            t2 = self.target_projector(self.target_encoder(x2))\n",
    "        return p1, p2, t1, t2\n",
    "\n",
    "    def update_target_network(self, momentum=0.996):\n",
    "        for p_o, p_t in zip(self.online_encoder.parameters(), self.target_encoder.parameters()):\n",
    "            p_t.data = p_t.data * momentum + p_o.data * (1. - momentum)\n",
    "        for p_o, p_t in zip(self.online_projector.parameters(), self.target_projector.parameters()):\n",
    "            p_t.data = p_t.data * momentum + p_o.data * (1. - momentum)\n",
    "\n",
    "# BYOL loss\n",
    "def byol_loss(p, t):\n",
    "    p = F.normalize(p, dim=-1)\n",
    "    t = F.normalize(t, dim=-1)\n",
    "    return 2 - 2 * (p * t).sum(dim=-1).mean()\n",
    "\n",
    "# Pretrain\n",
    "byol_model = BYOL(BYOLResNet().to(DEVICE), 128, 512).to(DEVICE)\n",
    "optimizer = optim.Adam(byol_model.parameters(), lr=1e-3)\n",
    "print(\"Pretraining BYOL...\")\n",
    "for epoch in range(10):\n",
    "    byol_model.train()\n",
    "    total_loss = 0\n",
    "    for x1, x2 in tqdm(train_loader_byol):\n",
    "        x1, x2 = x1.to(DEVICE), x2.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        p1, p2, t1, t2 = byol_model(x1, x2)\n",
    "        loss = byol_loss(p1, t2) + byol_loss(p2, t1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        byol_model.update_target_network()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader_byol):.4f}\")\n",
    "\n",
    "torch.save(byol_model.online_encoder.state_dict(), \"byol_encoder.pth\")\n",
    "\n",
    "# Linear evaluation\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x): return self.fc(x)\n",
    "\n",
    "encoder = BYOLResNet().to(DEVICE)\n",
    "encoder.load_state_dict(torch.load(\"byol_encoder.pth\"))\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "classifier = LinearClassifier(encoder.feature_dim, NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training classifier...\")\n",
    "for epoch in range(10):\n",
    "    classifier.train()\n",
    "    for images, labels in train_loader_eval:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = encoder(images)\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "classifier.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader_eval:\n",
    "        images = images.to(DEVICE)\n",
    "        features = encoder(images)\n",
    "        outputs = classifier(features)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
