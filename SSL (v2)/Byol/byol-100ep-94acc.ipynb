{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2f0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SimCLR + End-to-End Fine-Tune (no freezing) ===\n",
    "# With EMA (fine-tune), MixUp, and TTA + Curves (fp16-safe)\n",
    "import os, math, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "DATA_DIR = '/kaggle/input/riceds-original/Original'\n",
    "SAVE_DIR = './'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "SEED = 42\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# SimCLR pretrain\n",
    "SIMCLR_EPOCHS = 100        # try 200â€“400 if you can\n",
    "SIMCLR_LR = 3e-4\n",
    "SIMCLR_WEIGHT_DECAY = 1e-6\n",
    "TEMPERATURE = 0.2\n",
    "\n",
    "# Supervised fine-tune (NO FREEZING)\n",
    "FINETUNE_EPOCHS = 30\n",
    "FT_LR_BACKBONE = 1e-4      # lower LR for encoder\n",
    "FT_LR_HEAD = 1e-3          # higher LR for classifier head\n",
    "FT_WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTH = 0.0         # if using MixUp, keep 0 or very small\n",
    "\n",
    "USE_IMAGENET_WEIGHTS = True\n",
    "\n",
    "# Extras\n",
    "USE_EMA = True             # EMA of fine-tune weights\n",
    "EMA_DECAY = 0.999\n",
    "USE_MIXUP = True\n",
    "MIXUP_ALPHA = 0.2\n",
    "USE_TTA = True             # eval-time flip TTA\n",
    "\n",
    "# Histories for curves\n",
    "simclr_loss_hist = []\n",
    "ft_loss_hist = []\n",
    "ft_acc_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "# -----------------------\n",
    "# Augmentations\n",
    "# -----------------------\n",
    "class TwoCropsTransform:\n",
    "    \"\"\"Return two random augmentations of the same image (SimCLR).\"\"\"\n",
    "    def __init__(self, size=224):\n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                         [0.229, 0.224, 0.225])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size, scale=(0.08, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    def __call__(self, x):\n",
    "        return self.transform(x), self.transform(x)\n",
    "\n",
    "supervised_train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# Stratified 80/20 split BEFORE training\n",
    "# -----------------------\n",
    "_for_split = datasets.ImageFolder(DATA_DIR, transform=transforms.ToTensor())\n",
    "labels_all = [lbl for _, lbl in _for_split.samples]\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_idx, test_idx = next(sss.split(np.zeros(len(labels_all)), labels_all))\n",
    "\n",
    "# -----------------------\n",
    "# SSL datasets/loaders with SAME indices\n",
    "# -----------------------\n",
    "ssl_dataset = datasets.ImageFolder(DATA_DIR, transform=TwoCropsTransform())\n",
    "train_ssl = Subset(ssl_dataset, train_idx)\n",
    "\n",
    "train_loader_ssl = DataLoader(\n",
    "    train_ssl,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,                 # BN stability\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Encoder & Heads\n",
    "# -----------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_imagenet=True):\n",
    "        super().__init__()\n",
    "        if use_imagenet:\n",
    "            try:\n",
    "                base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "            except Exception:\n",
    "                base = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            base = models.resnet50(weights=None)\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])  # (B, 2048, 1, 1)\n",
    "        self.feature_dim = 2048\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return torch.flatten(x, 1)                                   # (B, 2048)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, bn_last=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "        ]\n",
    "        if bn_last:\n",
    "            layers.append(nn.BatchNorm1d(out_dim, affine=False))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------\n",
    "# SimCLR model\n",
    "# -----------------------\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, encoder, proj_dim=256, hidden=2048):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.projector = MLP(encoder.feature_dim, hidden, proj_dim, bn_last=True)\n",
    "    def forward(self, x1, x2):\n",
    "        h1 = self.encoder(x1)\n",
    "        h2 = self.encoder(x2)\n",
    "        z1 = self.projector(h1)\n",
    "        z2 = self.projector(h2)\n",
    "        return z1, z2\n",
    "\n",
    "# ---- FP16-safe InfoNCE (compute logits in float32) ----\n",
    "def nt_xent_loss(z1, z2, temperature=0.2):\n",
    "    \"\"\"\n",
    "    Compute SimCLR NT-Xent loss in float32 to avoid fp16 overflow on large negative masks.\n",
    "    \"\"\"\n",
    "    b = z1.size(0)\n",
    "    z1 = F.normalize(z1, dim=1).float()\n",
    "    z2 = F.normalize(z2, dim=1).float()\n",
    "    z = torch.cat([z1, z2], dim=0)                                   # (2B, d) float32\n",
    "    sim = torch.matmul(z, z.T) / float(temperature)                  # (2B, 2B) float32\n",
    "    mask = torch.eye(2*b, device=z.device, dtype=torch.bool)\n",
    "    sim = sim.masked_fill(mask, float('-inf'))\n",
    "    targets = torch.arange(2*b, device=z.device)\n",
    "    targets = (targets + b) % (2*b)\n",
    "    loss = F.cross_entropy(sim, targets)                             # fp32 CE\n",
    "    return loss\n",
    "\n",
    "# -----------------------\n",
    "# SimCLR Pretraining\n",
    "# -----------------------\n",
    "encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n",
    "simclr = SimCLR(encoder).to(DEVICE)\n",
    "\n",
    "ssl_optimizer = torch.optim.AdamW(simclr.parameters(), lr=SIMCLR_LR, weight_decay=SIMCLR_WEIGHT_DECAY)\n",
    "ssl_sched = torch.optim.lr_scheduler.CosineAnnealingLR(ssl_optimizer, T_max=SIMCLR_EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "for epoch in range(SIMCLR_EPOCHS):\n",
    "    simclr.train()\n",
    "    running = 0.0\n",
    "    pbar = tqdm(train_loader_ssl, desc=f\"SimCLR Epoch {epoch+1}/{SIMCLR_EPOCHS}\")\n",
    "    for (v1, v2), _ in pbar:                                          # ((view1, view2), label)\n",
    "        x1 = v1.to(DEVICE, non_blocking=True)\n",
    "        x2 = v2.to(DEVICE, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "            z1, z2 = simclr(x1, x2)\n",
    "            loss = nt_xent_loss(z1, z2, temperature=TEMPERATURE)\n",
    "        ssl_optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(ssl_optimizer)\n",
    "        scaler.update()\n",
    "        running += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{running / (pbar.n or 1):.4f}\")\n",
    "    epoch_loss = running / len(train_loader_ssl)\n",
    "    simclr_loss_hist.append(epoch_loss)\n",
    "    ssl_sched.step()\n",
    "    print(f\"SimCLR Epoch {epoch+1}: loss={epoch_loss:.4f}\")\n",
    "\n",
    "# Save encoder after SimCLR pretrain\n",
    "enc_path = os.path.join(SAVE_DIR, \"simclr_encoder.pth\")\n",
    "torch.save(simclr.encoder.state_dict(), enc_path)\n",
    "print(f\"Saved SimCLR encoder to: {enc_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# Supervised Fine-Tuning (NO FREEZING) + EMA + MixUp\n",
    "# -----------------------\n",
    "sup_train_ds = datasets.ImageFolder(DATA_DIR, transform=supervised_train_tf)\n",
    "sup_test_ds  = datasets.ImageFolder(DATA_DIR, transform=eval_tf)\n",
    "num_classes = len(sup_train_ds.classes)\n",
    "\n",
    "train_sup = Subset(sup_train_ds, train_idx)\n",
    "test_sup  = Subset(sup_test_ds,  test_idx)\n",
    "\n",
    "train_loader_sup = DataLoader(\n",
    "    train_sup, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "test_loader_sup = DataLoader(\n",
    "    test_sup, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "class SupModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Linear(encoder.feature_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)                                       # (B, 2048)\n",
    "        return self.head(feats)\n",
    "\n",
    "finetune_encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n",
    "finetune_encoder.load_state_dict(torch.load(enc_path, map_location=DEVICE))\n",
    "sup_model = SupModel(finetune_encoder, num_classes).to(DEVICE)\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": sup_model.encoder.parameters(), \"lr\": FT_LR_BACKBONE, \"weight_decay\": FT_WEIGHT_DECAY},\n",
    "    {\"params\": sup_model.head.parameters(),    \"lr\": FT_LR_HEAD,     \"weight_decay\": FT_WEIGHT_DECAY},\n",
    "]\n",
    "ft_optimizer = torch.optim.AdamW(param_groups)\n",
    "ft_sched = torch.optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max=FINETUNE_EPOCHS)\n",
    "\n",
    "# --- MixUp helpers ---\n",
    "def one_hot_with_smoothing(y, num_classes, eps=0.0):\n",
    "    y = y.view(-1)\n",
    "    oh = torch.zeros(y.size(0), num_classes, device=y.device)\n",
    "    oh.scatter_(1, y.unsqueeze(1), 1.0)\n",
    "    if eps > 0:\n",
    "        oh = oh * (1 - eps) + eps / num_classes\n",
    "    return oh\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, num_classes=1000, eps=0.0):\n",
    "    if alpha <= 0:\n",
    "        return x, one_hot_with_smoothing(y, num_classes, eps), 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    index = torch.randperm(x.size(0), device=x.device)\n",
    "    x_mix = lam * x + (1 - lam) * x[index]\n",
    "    y1 = one_hot_with_smoothing(y, num_classes, eps)\n",
    "    y2 = y1[index]\n",
    "    y_mix = lam * y1 + (1 - lam) * y2\n",
    "    return x_mix, y_mix, lam\n",
    "\n",
    "def soft_cross_entropy(logits, target_prob):\n",
    "    log_prob = F.log_softmax(logits, dim=1)\n",
    "    return -(target_prob * log_prob).sum(dim=1).mean()\n",
    "\n",
    "# --- EMA for fine-tune ---\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone()\n",
    "                       for k, v in model.state_dict().items()\n",
    "                       if v.dtype.is_floating_point}\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for k, v in model.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1 - self.decay)\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model):\n",
    "        sd = model.state_dict()\n",
    "        for k, v in self.shadow.items():\n",
    "            sd[k].copy_(v)\n",
    "\n",
    "ema = EMA(sup_model, decay=EMA_DECAY) if USE_EMA else None\n",
    "\n",
    "scaler_ft = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "def accuracy_top1(logits, targets):\n",
    "    return (logits.argmax(dim=1) == targets).float().mean().item()\n",
    "\n",
    "def predict_tta(model, x):\n",
    "    # flip TTA\n",
    "    logits1 = model(x)\n",
    "    logits2 = model(torch.flip(x, dims=[3]))  # horizontal flip\n",
    "    return (logits1 + logits2) / 2\n",
    "\n",
    "best_acc = 0.0\n",
    "best_path = os.path.join(SAVE_DIR, \"simclr_finetune_best.pt\")\n",
    "\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    sup_model.train()\n",
    "    run_loss, run_acc = 0.0, 0.0\n",
    "    for x, y in tqdm(train_loader_sup, desc=f\"FT Epoch {epoch+1}/{FINETUNE_EPOCHS}\"):\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if USE_MIXUP:\n",
    "            x_in, y_soft, _ = mixup_data(x, y, alpha=MIXUP_ALPHA, num_classes=num_classes, eps=LABEL_SMOOTH)\n",
    "        else:\n",
    "            x_in, y_soft = x, one_hot_with_smoothing(y, num_classes, eps=LABEL_SMOOTH)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "            logits = sup_model(x_in)\n",
    "            loss = soft_cross_entropy(logits, y_soft)\n",
    "\n",
    "        ft_optimizer.zero_grad(set_to_none=True)\n",
    "        scaler_ft.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(sup_model.parameters(), 5.0)\n",
    "        scaler_ft.step(ft_optimizer)\n",
    "        scaler_ft.update()\n",
    "\n",
    "        if USE_EMA:\n",
    "            ema.update(sup_model)\n",
    "\n",
    "        # track training accuracy on *non-mixed* view for readability\n",
    "        with torch.no_grad():\n",
    "            logits_nomix = sup_model(x)\n",
    "            run_acc += accuracy_top1(logits_nomix, y)\n",
    "\n",
    "        run_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = run_loss / len(train_loader_sup)\n",
    "    epoch_train_acc  = run_acc  / len(train_loader_sup)\n",
    "    ft_loss_hist.append(epoch_train_loss)\n",
    "    ft_acc_hist.append(epoch_train_acc)\n",
    "    ft_sched.step()\n",
    "    print(f\"[FT] Epoch {epoch+1}: loss={epoch_train_loss:.4f} | acc={epoch_train_acc:.4f}\")\n",
    "\n",
    "    # --- Eval each epoch with EMA (if enabled) + TTA ---\n",
    "    sup_model.eval()\n",
    "    # backup current floating weights\n",
    "    backup = {k: v.detach().clone() for k, v in sup_model.state_dict().items() if v.dtype.is_floating_point}\n",
    "    if USE_EMA:\n",
    "        ema.copy_to(sup_model)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader_sup:\n",
    "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "            if USE_TTA:\n",
    "                logits = predict_tta(sup_model, x)\n",
    "            else:\n",
    "                logits = sup_model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    epoch_acc = correct / max(total, 1)\n",
    "    val_acc_hist.append(epoch_acc)\n",
    "\n",
    "    # restore original weights after eval\n",
    "    sd = sup_model.state_dict()\n",
    "    for k, v in backup.items():\n",
    "        sd[k].copy_(v)\n",
    "\n",
    "    # save best (EMA-TTA) checkpoint\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        # save an EMA snapshot if enabled, else current weights\n",
    "        if USE_EMA:\n",
    "            ema.copy_to(sup_model)\n",
    "            torch.save(sup_model.state_dict(), best_path)\n",
    "            # restore again for next epoch training\n",
    "            for k, v in backup.items():\n",
    "                sd[k].copy_(v)\n",
    "        else:\n",
    "            torch.save(sup_model.state_dict(), best_path)\n",
    "\n",
    "    print(f\"[Eval] top1={epoch_acc:.4f} (best={best_acc:.4f})\")\n",
    "\n",
    "# -----------------------\n",
    "# Final evaluation on held-out 20% (using best checkpoint, EMA if enabled)\n",
    "# -----------------------\n",
    "# Load fresh model for clean eval\n",
    "finetune_encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n",
    "finetune_encoder.load_state_dict(torch.load(enc_path, map_location=DEVICE))\n",
    "sup_model = SupModel(finetune_encoder, num_classes).to(DEVICE)\n",
    "sup_model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "sup_model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_sup:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        logits = predict_tta(sup_model, x) if USE_TTA else sup_model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y.numpy())\n",
    "\n",
    "print(\"\\n=== SimCLR -> End-to-End Fine-Tune: Held-out Test ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(\"Confusion Matrix (SimCLR Fine-Tune, EMA+MixUp+TTA)\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(SAVE_DIR, \"simclr_confusion_matrix.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# Curves: pretrain loss & fine-tune loss/accuracy\n",
    "# -----------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(simclr_loss_hist, marker='o')\n",
    "plt.title(\"SimCLR Pretraining Loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"simclr_pretrain_loss.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18,4))\n",
    "ax[0].plot(ft_loss_hist, marker='o')\n",
    "ax[0].set_title(\"Fine-tune Loss\"); ax[0].set_xlabel(\"Epoch\"); ax[0].set_ylabel(\"Loss\"); ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax[1].plot(ft_acc_hist, marker='o', label='Train')\n",
    "ax[1].plot(val_acc_hist, marker='s', label='Val (EMA+TTA)')\n",
    "ax[1].set_title(\"Accuracy\"); ax[1].set_xlabel(\"Epoch\"); ax[1].set_ylabel(\"Top-1 Acc\"); ax[1].grid(True, alpha=0.3); ax[1].legend()\n",
    "\n",
    "ax[2].plot(simclr_loss_hist, marker='o')\n",
    "ax[2].set_title(\"Pretrain Loss\"); ax[2].set_xlabel(\"Epoch\"); ax[2].set_ylabel(\"Loss\"); ax[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(SAVE_DIR, \"simclr_training_curves.png\"), dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910fe64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67c34f7f",
   "metadata": {},
   "source": [
    "\n",
    "=== End-to-End Fine-Tune Evaluation (Held-out 20%) ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.8704    0.9400    0.9038       100\n",
    "           1     0.9468    0.9271    0.9368        96\n",
    "           2     1.0000    1.0000    1.0000       109\n",
    "           3     0.8941    0.7917    0.8398        96\n",
    "           4     0.9884    1.0000    0.9942        85\n",
    "           5     0.9189    0.9623    0.9401       106\n",
    "           6     0.9268    0.9661    0.9461       118\n",
    "           7     1.0000    1.0000    1.0000        92\n",
    "           8     0.9870    0.9500    0.9682        80\n",
    "           9     0.9429    1.0000    0.9706        99\n",
    "          10     0.9121    0.8737    0.8925        95\n",
    "          11     1.0000    0.9727    0.9862       110\n",
    "          12     0.9391    0.9818    0.9600       110\n",
    "          13     0.9205    0.9643    0.9419        84\n",
    "          14     0.9895    0.9792    0.9843        96\n",
    "          15     0.9905    0.9811    0.9858       106\n",
    "          16     0.9890    0.9375    0.9626        96\n",
    "          17     1.0000    1.0000    1.0000        94\n",
    "          18     1.0000    0.9286    0.9630        98\n",
    "          19     0.9818    1.0000    0.9908       108\n",
    "          20     0.9783    0.9890    0.9836        91\n",
    "          21     0.9792    0.9792    0.9792        96\n",
    "          22     1.0000    1.0000    1.0000       101\n",
    "          23     0.8191    0.9167    0.8652        84\n",
    "          24     0.8862    0.9820    0.9316       111\n",
    "          25     0.9038    0.9691    0.9353        97\n",
    "          26     0.9118    0.8158    0.8611       114\n",
    "          27     0.8509    0.9417    0.8940       103\n",
    "          28     0.9524    0.9091    0.9302       110\n",
    "          29     0.9792    0.9216    0.9495       102\n",
    "          30     1.0000    1.0000    1.0000       104\n",
    "          31     0.9570    0.8165    0.8812       109\n",
    "          32     0.9478    0.9732    0.9604       112\n",
    "          33     0.8989    0.8511    0.8743        94\n",
    "          34     0.9817    1.0000    0.9907       107\n",
    "          35     0.8246    0.9592    0.8868        98\n",
    "          36     0.9294    0.8587    0.8927        92\n",
    "          37     0.9540    0.8557    0.9022        97\n",
    "\n",
    "    accuracy                         0.9447      3800\n",
    "   macro avg     0.9461    0.9446    0.9443      3800\n",
    "weighted avg     0.9463    0.9447    0.9445      3800"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
