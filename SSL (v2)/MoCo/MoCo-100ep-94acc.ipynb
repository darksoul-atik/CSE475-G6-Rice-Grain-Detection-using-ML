{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MoCo (v2-style) + End-to-End Fine-Tune (no freezing) ===\n",
    "import os, copy, math, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "DATA_DIR = '/kaggle/input/riceds-original/Original'\n",
    "SAVE_DIR = './'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# === MoCo pretrain ===\n",
    "MOCO_EPOCHS = 100          # increase if possible (e.g., 200â€“400)\n",
    "SSL_LR = 3e-4\n",
    "SSL_WEIGHT_DECAY = 1e-6\n",
    "# MoCo hyperparams\n",
    "MOCO_DIM = 256             # projection dimension\n",
    "MOCO_K = 16384             # queue size (power of 2 is nice)\n",
    "MOCO_M = 0.996             # EMA/momentum for key encoder\n",
    "MOCO_T = 0.2               # softmax temperature\n",
    "\n",
    "# Supervised fine-tune (NO FREEZING)\n",
    "FINETUNE_EPOCHS = 30\n",
    "FT_LR_BACKBONE = 1e-4\n",
    "FT_LR_HEAD = 1e-3\n",
    "FT_WEIGHT_DECAY = 1e-4\n",
    "LABEL_SMOOTH = 0.1\n",
    "\n",
    "USE_IMAGENET_WEIGHTS = True\n",
    "NUM_WORKERS = 4\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Augmentations\n",
    "# -----------------------\n",
    "class TwoCropsTransform:\n",
    "    \"\"\"Two strong views per image (MoCo v2-like).\"\"\"\n",
    "    def __init__(self, size=224):\n",
    "        normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                         [0.229, 0.224, 0.225])\n",
    "        self.base = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size, scale=(0.2, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    def __call__(self, x):\n",
    "        return self.base(x), self.base(x)\n",
    "\n",
    "# weaker augs for supervised training\n",
    "supervised_train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "eval_tf = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# Dataset & 80/20 split BEFORE training\n",
    "# -----------------------\n",
    "full_ssl = datasets.ImageFolder(DATA_DIR, transform=TwoCropsTransform())\n",
    "num_total = len(full_ssl)\n",
    "num_train = int(0.8 * num_total)\n",
    "num_test = num_total - num_train\n",
    "\n",
    "train_subset, test_subset = random_split(\n",
    "    full_ssl, [num_train, num_test],\n",
    "    generator=torch.Generator().manual_seed(SEED)\n",
    ")\n",
    "\n",
    "train_indices = train_subset.indices\n",
    "test_indices  = test_subset.indices\n",
    "\n",
    "train_loader_ssl = DataLoader(\n",
    "    Subset(full_ssl, train_indices),\n",
    "    batch_size=BATCH_SIZE, shuffle=True, drop_last=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Encoder (ResNet-50 trunk)\n",
    "# -----------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, use_imagenet=True):\n",
    "        super().__init__()\n",
    "        if use_imagenet:\n",
    "            try:\n",
    "                base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "            except Exception:\n",
    "                base = models.resnet50(pretrained=True)\n",
    "        else:\n",
    "            base = models.resnet50(weights=None)\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])  # (B, 2048, 1, 1)\n",
    "        self.feature_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "# -----------------------\n",
    "# MLP Head (MoCo v2 uses 2-layer MLP)\n",
    "# -----------------------\n",
    "class ProjectionMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------\n",
    "# === MoCo model ===\n",
    "# -----------------------\n",
    "class MoCo(nn.Module):\n",
    "    \"\"\"\n",
    "    Momentum Contrast (v2-style): query encoder, key encoder (EMA), queue.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, dim=128, K=65536, m=0.999, T=0.07):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.T = T\n",
    "\n",
    "        # encoders\n",
    "        self.encoder_q = base_encoder\n",
    "        self.encoder_k = copy.deepcopy(base_encoder)\n",
    "        for p in self.encoder_k.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # projection heads\n",
    "        self.proj_q = ProjectionMLP(self.encoder_q.feature_dim, 2048, dim)\n",
    "        self.proj_k = copy.deepcopy(self.proj_q)\n",
    "        for p in self.proj_k.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # queue (features are L2-normalized)\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, K))\n",
    "        self.queue = F.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"EMA update for key encoder and projector.\"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "        for param_q, param_k in zip(self.proj_q.parameters(), self.proj_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        # gather keys before updating queue\n",
    "        batch_size = keys.shape[0]\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.K % batch_size == 0, \"Queue size must be divisible by batch size for simplicity.\"\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.T\n",
    "        ptr = (ptr + batch_size) % self.K\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    def forward(self, im_q, im_k):\n",
    "        # compute query features\n",
    "        q = self.encoder_q(im_q)\n",
    "        q = self.proj_q(q)\n",
    "        q = F.normalize(q, dim=1)  # (B, dim)\n",
    "\n",
    "        # compute key features\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update_key_encoder()\n",
    "            k = self.encoder_k(im_k)\n",
    "            k = self.proj_k(k)\n",
    "            k = F.normalize(k, dim=1)\n",
    "\n",
    "        # positive logits: q @ k\n",
    "        l_pos = torch.einsum('nd,nd->n', [q, k]).unsqueeze(-1)  # (B, 1)\n",
    "        # negative logits: q @ queue\n",
    "        l_neg = torch.einsum('nd,dk->nk', [q, self.queue.clone().detach()])  # (B, K)\n",
    "\n",
    "        logits = torch.cat([l_pos, l_neg], dim=1)    # (B, 1+K)\n",
    "        logits /= self.T\n",
    "\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)  # positives are 0\n",
    "\n",
    "        # dequeue and enqueue\n",
    "        with torch.no_grad():\n",
    "            self._dequeue_and_enqueue(k)\n",
    "\n",
    "        return logits, labels\n",
    "\n",
    "# -----------------------\n",
    "# MoCo Pretraining\n",
    "# -----------------------\n",
    "encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n",
    "model = MoCo(\n",
    "    base_encoder=encoder,\n",
    "    dim=MOCO_DIM, K=MOCO_K, m=MOCO_M, T=MOCO_T\n",
    ").to(DEVICE)\n",
    "\n",
    "ssl_optimizer = torch.optim.AdamW(\n",
    "    list(model.encoder_q.parameters()) + list(model.proj_q.parameters()),\n",
    "    lr=SSL_LR, weight_decay=SSL_WEIGHT_DECAY\n",
    ")\n",
    "ssl_sched = torch.optim.lr_scheduler.CosineAnnealingLR(ssl_optimizer, T_max=MOCO_EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "criterion_ssl = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(MOCO_EPOCHS):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    pbar = tqdm(train_loader_ssl, desc=f\"MoCo Epoch {epoch+1}/{MOCO_EPOCHS}\")\n",
    "    for (v1, v2), _ in pbar:\n",
    "        x_q = v1.to(DEVICE, non_blocking=True)  # query view\n",
    "        x_k = v2.to(DEVICE, non_blocking=True)  # key view\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "            logits, labels = model(x_q, x_k)\n",
    "            loss = criterion_ssl(logits, labels)\n",
    "\n",
    "        ssl_optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(ssl_optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running += loss.item()\n",
    "        pbar.set_postfix(loss=f\"{running / (pbar.n or 1):.4f}\")\n",
    "\n",
    "    ssl_sched.step()\n",
    "    print(f\"MoCo Epoch {epoch+1}: loss={running / len(train_loader_ssl):.4f}\")\n",
    "\n",
    "# Save encoder weights (query encoder after MoCo pretrain)\n",
    "encoder_path = os.path.join(SAVE_DIR, \"moco_encoder.pth\")\n",
    "torch.save(model.encoder_q.state_dict(), encoder_path)\n",
    "print(f\"Saved pretrain encoder to: {encoder_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# Supervised Fine-Tuning (NO FREEZING)\n",
    "#   - Use SAME 80/20 indices\n",
    "# -----------------------\n",
    "full_sup_train = datasets.ImageFolder(DATA_DIR, transform=supervised_train_tf)\n",
    "full_sup_test  = datasets.ImageFolder(DATA_DIR, transform=eval_tf)\n",
    "num_classes = len(full_sup_train.classes)\n",
    "\n",
    "train_sup = Subset(full_sup_train, train_indices)\n",
    "test_sup  = Subset(full_sup_test,  test_indices)\n",
    "\n",
    "train_loader_sup = DataLoader(\n",
    "    train_sup, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "test_loader_sup = DataLoader(\n",
    "    test_sup, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "# Build encoder and load MoCo-pretrained weights\n",
    "finetune_encoder = Encoder(use_imagenet=USE_IMAGENET_WEIGHTS).to(DEVICE)\n",
    "finetune_encoder.load_state_dict(torch.load(encoder_path, map_location=DEVICE))\n",
    "\n",
    "# Classifier head\n",
    "class SupModel(nn.Module):\n",
    "    def __init__(self, encoder, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Linear(encoder.feature_dim, num_classes)\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)\n",
    "        return self.head(feats)\n",
    "\n",
    "sup_model = SupModel(finetune_encoder, num_classes).to(DEVICE)\n",
    "\n",
    "# Optimizer with differential LR\n",
    "param_groups = [\n",
    "    {\"params\": sup_model.encoder.parameters(), \"lr\": FT_LR_BACKBONE, \"weight_decay\": FT_WEIGHT_DECAY},\n",
    "    {\"params\": sup_model.head.parameters(),    \"lr\": FT_LR_HEAD,      \"weight_decay\": FT_WEIGHT_DECAY},\n",
    "]\n",
    "ft_optimizer = torch.optim.AdamW(param_groups)\n",
    "ft_sched = torch.optim.lr_scheduler.CosineAnnealingLR(ft_optimizer, T_max=FINETUNE_EPOCHS)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "scaler_ft = torch.cuda.amp.GradScaler(enabled=(DEVICE.type == 'cuda'))\n",
    "\n",
    "def accuracy_top1(logits, targets):\n",
    "    pred = logits.argmax(dim=1)\n",
    "    return (pred == targets).float().mean().item()\n",
    "\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    sup_model.train()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    for x, y in tqdm(train_loader_sup, desc=f\"FT Epoch {epoch+1}/{FINETUNE_EPOCHS}\"):\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type == 'cuda')):\n",
    "            logits = sup_model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        ft_optimizer.zero_grad(set_to_none=True)\n",
    "        scaler_ft.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(sup_model.parameters(), max_norm=5.0)\n",
    "        scaler_ft.step(ft_optimizer)\n",
    "        scaler_ft.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_acc += accuracy_top1(logits, y)\n",
    "\n",
    "    ft_sched.step()\n",
    "    print(f\"[FT] Epoch {epoch+1}: loss={running_loss/len(train_loader_sup):.4f} | \"\n",
    "          f\"acc={running_acc/len(train_loader_sup):.4f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Evaluation on held-out 20%\n",
    "# -----------------------\n",
    "sup_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_sup:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        logits = sup_model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y.numpy())\n",
    "\n",
    "print(\"\\n=== End-to-End Fine-Tune Evaluation (Held-out 20%) ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(\"Confusion Matrix (End-to-End Fine-Tune)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d934f",
   "metadata": {},
   "source": [
    "=== End-to-End Fine-Tune Evaluation (Held-out 20%) ===\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0     0.9048    0.9500    0.9268       100\n",
    "           1     0.9184    0.9375    0.9278        96\n",
    "           2     1.0000    0.9908    0.9954       109\n",
    "           3     0.9277    0.8021    0.8603        96\n",
    "           4     0.9655    0.9882    0.9767        85\n",
    "           5     0.8938    0.9528    0.9224       106\n",
    "           6     0.8810    0.9407    0.9098       118\n",
    "           7     1.0000    0.9891    0.9945        92\n",
    "           8     0.9630    0.9750    0.9689        80\n",
    "           9     0.9515    0.9899    0.9703        99\n",
    "          10     0.8817    0.8632    0.8723        95\n",
    "          11     0.9910    1.0000    0.9955       110\n",
    "          12     0.8793    0.9273    0.9027       110\n",
    "          13     0.9268    0.9048    0.9157        84\n",
    "          14     0.9896    0.9896    0.9896        96\n",
    "          15     0.9806    0.9528    0.9665       106\n",
    "          16     0.9333    0.8750    0.9032        96\n",
    "          17     0.9894    0.9894    0.9894        94\n",
    "          18     1.0000    0.9184    0.9574        98\n",
    "          19     0.9908    1.0000    0.9954       108\n",
    "          20     0.9889    0.9780    0.9834        91\n",
    "          21     0.9688    0.9688    0.9688        96\n",
    "          22     1.0000    1.0000    1.0000       101\n",
    "          23     0.8152    0.8929    0.8523        84\n",
    "          24     0.8730    0.9910    0.9283       111\n",
    "          25     0.9596    0.9794    0.9694        97\n",
    "          26     0.9320    0.8421    0.8848       114\n",
    "          27     0.8870    0.9903    0.9358       103\n",
    "          28     0.9903    0.9273    0.9577       110\n",
    "          29     0.9796    0.9412    0.9600       102\n",
    "          30     1.0000    1.0000    1.0000       104\n",
    "          31     0.9184    0.8257    0.8696       109\n",
    "          32     0.9474    0.9643    0.9558       112\n",
    "          33     0.9506    0.8191    0.8800        94\n",
    "          34     0.9907    0.9907    0.9907       107\n",
    "          35     0.8785    0.9592    0.9171        98\n",
    "          36     0.9062    0.9457    0.9255        92\n",
    "          37     0.9121    0.8557    0.8830        97\n",
    "\n",
    "    accuracy                         0.9426      3800\n",
    "   macro avg     0.9438    0.9423    0.9422      3800\n",
    "weighted avg     0.9440    0.9426    0.9424      3800\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
